%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

\usepackage[utf8]{inputenc}

% choose options for [] as required from the list
% in the Reference Guide

\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom


\usepackage{newtxtext}       % 
\usepackage{newtxmath}       % selects Times Roman as basic font

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{subfig}

% see the list of further useful packages
% in the Reference Guide

\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title*{Model Reduction Methods}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\author{Miklós F. Hatwagner}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\institute{Miklós F. Hatwagner \at Széchenyi István University, Győr, Hungary \email{miklos.hatwagner@sze.hu}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle

\newcommand{\sameabstract}{Fuzzy Cognitive Maps are very useful tools primarily in decision 
making and management tasks. They represent the main factors, variables of a complex 
system and the internal causal relationships among them in a straightforward 
way. Simulations can be started with an initial state, and the future 
states of the system under investigation can be predicted. This way, 
what-if questions can be answered. If the model of a system is created 
by experts they are often tempted to include too many
components, because they are not sure in the importance of them. An oversized 
model is excruciating to use in practice, however. Model reduction 
methods help to decrease model size but unavoidably cause information 
loss as well. This effect does not cause a problem in practical decision making applications if the model 
suggests the same decisions. This chapter covers three FCM modell 
reduction methods, their theoretical background and behavioral 
properties.}

\abstract*{\sameabstract}

\abstract{\sameabstract}

\section{Introduction}
\label{sec:introduction}

In Chapter 3 the initial FCM model of a Sustainable Waste Management System \cite{buruzs2013developing} was created, which contained six concepts. These concepts were identified on the basis of consensus among the stakeholders of the field found in the relevant literature. The strength of relationships among concepts were defined by the results of a survey filled out by 75 stakeholders.

Not only the data of the model, but time series data were also collected based on the literature. It served as the input of a Bacterial Evolutionary Algorithm (BEA) to learn the connection weights among the already specified concepts and parameter $\lambda$ of the threshold function. This time series data could be compared to the time series generated by simulating the dynamic behavior of the initial model. The validation of the model highlighted \cite{buruzs2013advanced} that the time series generated by the initial model is far from the time series collected from the literature, and the model created by BEA based on time series found in literature does not resembles the initial model. Therefore, a contradiction was found between the available time series data and the initial model.

In order to resolve the experienced problem the concepts of the initial model were decomposed to further 4-7 sub-concepts according to the System-of-Systems approach as described in Chapter 4. It led to a very detailed, completely new model \cite{buruzs2013modeling}. A workshop was organized with the help of 12 stakeholders who decided the sub-concepts and their interconnections. The result of their work is a FCM containing 33 concepts in total (Fig.~\ref{fig:flower}). Unfortunately such an extremely complex model is often confuses experts (Fig.~\ref{fig:fcmbig}), and to work with them may be very laborious. Note that the number of connections is a quadratic function of the number of concepts.

Time series data was also collected for the new model as Chapter 5 shows it. This model is much more complex than the initial one thus the method of collecting time series data was also different: it was based on text mining. Several types of documents including EU and domestic legislation, directives, management plans and strategies was analysed covering the time interval from the 1970's to 2014.

The new model solved the problem of model inaccuracy and eliminated the contradiction but it is really hard or even impossible to work with such a huge model in practice. It worth noting that the new model contains 638 connections which practically cannot be handled by experts. That is why, in general, the following approach is suggested to follow in practice: start with an obviously oversized, fine-grained model. Experts are often uncertain about the importance of system components thus it worth include most or all of them in the preliminary model. Then start reducing the model automatically, in an algorithmic way until the balance of model size and required accuracy is found. The numerical accuracy of reduced models are always lower by their nature, but it does not cause a problem in practice until the decisions suggested by them are the same. On the other hand, simpler models are easier to understand, their visual representation is clearer, and it is often more important for experts and managers. In the following sections several possible ways of model reduction is presented.

\begin{figure}[hbt]
  \begin{center}
    \includegraphics[width=\textwidth]{szines_virag.pdf}
  \end{center}
  \caption{The main concepts and their sub-concepts of regional IWMS.}
  \label{fig:flower}
\end{figure}

\begin{figure}[hbt]
  \begin{center}
    \includegraphics[width=\textwidth]{fcm_big.pdf}
  \end{center}
  \caption{The 33 concept model of regional IWMS and the relationships among its concepts. It is hard to illustrate complex FCM models appropriately and graphical model visualizations often confuse experts.}
  \label{fig:fcmbig}
\end{figure}

\section{Early model reduction methods}
\label{sec:earlyReduction}

Several methods had been suggested to solve the problem of oversized models before the complex model of IWMS saw the light of the day. These approaches are based on different perspectives.

In \cite{alizadeh2008using} an FCM is learned using historical data and its concepts are grouped into clusters in a unique way. The clustering is based on the DEMATEL \cite{dematel} method. The concepts are arranged on a two dimensional plot. The vertical axis classifies concepts to cause and effect groups, the position of concepts along the horizontal axis expresses the importance of them. Based on this rearrangement of concepts two clustering methods are suggested. The first one uses K-Means clustering to create clusters according to the cause-effect behavior of concepts. The cluster centers replace the original concepts in the reduced model. The second method contains two consecutive steps, and takes also the importance of concepts into consideration. Regardless of the methods applied, experts have to define the number of clusters and they must be disjoint.

FCM was used to predict and discover knowledge about the HIV-1 drug resistance in \cite{NapolesHIV}. The protease protein was modeled by FCM and causalities among sequence positions were estimated by Particle Swarm Optimization. Furthermore, Ant Colony Optimization was also applied in order to find the strongest sequence positions related to the resistance target. After that, some concepts and their connections were removed to decrease the complexity of the model until the quality of inference remained acceptable.

Another reduction method was introduced in \cite{Homenda2014}. Weak concepts and their connections were removed, then the inference capabilities of the simplified model were tested with time series data collected from real-world applications. The reduced model replaced the original if its estimation errors were acceptable.

\section{Fuzzy Tolerance Relations-based reduction methods}
\label{sec:FTR}

The first results on a novel state reduction method family were presented in \cite{hatwagner2014strategic,hatwagner2015new}. The members of this family differ only in the applied metric, which is used to measure the ``distance'' of concepts from each other. The methods may be considered as generalizations of the state reduction of finite state machines and sequential systems with partially defined states. They are widely applied in digital design where the complexity of the problem makes it possible \cite{kohaviz.jhan.k.2009}. The common idea is to create clusters of identical or similar concepts, and use these clusters instead of their members in the reduced model. The methods are based on Fuzzy Tolerance Relations (FTR), which is an extension of compatibility relations among crisp concepts. The methods examine the connection weights among concepts, because they define the effect on other concepts.

\subsection{Description of the main functions used for reduction}
\label{sec:reductionFunctions}

At the very beginning the new model contains exactly as many clusters as the number of concepts in the original model. These clusters are all disjoint single element sets containing one of the original concepts. In the following, the $i^{th}$ concept is denoted by $C_i$, and similarly, the $i^{th}$ cluster is denoted by $K_i$. The number of concepts is $n$. In the next steps further concepts are added to the clusters if they are ``close enough'' to each member of the cluster. The ``distance'' of concepts can be measured by various metrics. These metrics can be selected according to the specialities of the problem, and they differentiate the members of the algorithm family. Finally, some of these expanded clusters may be identical, containing exactly the same concepts. In order to keep clusters unique, only one of these clusters is retained.

One of the main functions is called $buildCluster$ (Algorithm~\ref{buildCluster}). Its goal is to create a cluster that initially contains only its $initialConcept$. Later this cluster will be expanded by merger of other concepts. The second parameter, $\epsilon$ specifies the maximum allowed distance between current cluster members and the concept under examination. The value of $\epsilon$ must be in the [0,~1] interval. This parameter plays an important role in model reduction, and have to be chosen properly by experts. Low values hardly reduce the size of the model, but high values may cause oversimplification. Its appropriate value is completely problem dependent.

\begin{algorithm}
  \caption{The \emph{buildCluster} function}\label{buildCluster}
  \begin{algorithmic}[1]
    \Function{buildCluster}{$initialConcept,\epsilon$}
    \State $K\gets \{initialConcept\}$
    \For{$i\gets0$; $i<n$; $i++$}
      \If{$i \not= initialConcept$}
        \State $member \gets true$
        \While{$member$ and \Call{hasNextElement}{$K$}}
          \State $j \gets$ \Call{nextElement}{$K$}
          \State $member \gets$ \Call{isNearA}{$j, i, \epsilon$}
        \EndWhile
        \If{$member$}
          \State $K \gets K + \{i\}$
        \EndIf
      \EndIf
    \EndFor
    \State \textbf{return} $K$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Function $isNearA$ is called several times in $buildCluster$ to decide whether the current concept $C_i$ can become a member of cluster $K$ or not. The number of function calls depends on how many member concepts do the cluster already have. This function (Algorithm~\ref{metricA}) implements one of the possible metrics, but can be replaced by any of the metrics presented here. The last one was suggested in \cite{hatwagnerm.f.koczyl.t.2015}.

\begin{algorithm}
  \caption{Function \emph{isNearA} implementing \emph{Metric ``A''}}\label{metricA}
  \begin{algorithmic}[1]
    \Function{isNearA}{$i, j, \epsilon$}
      \State $near \gets true$
      \For{$k\gets0$; $k<n$ and $near = true$; $k++$}
        \If{$k\not=i$ and $k\not=j$}
          \If{$\frac{|w_{i, k} - w_{j, k}|}{2} \geq \epsilon$ or 
              $\frac{|w_{k, i} - w_{k, j}|}{2} \geq \epsilon$} 
            \State $near \gets false$
          \EndIf
        \EndIf
      \EndFor
      \State \textbf{return} $near$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Metric ``A'' calculates the absolute difference of two connection weights, $w_{i,k}$ and $w_{j,k}$ (Fig.~\ref{fig:similarity}). The former one is the weight of connection between the cluster member candidate concept $C_i$ and an independent concept $C_k$. The latter one is the weight of connection between $C_j$, which is already a member of cluster $K$, and $C_k$. Here $i \ne j \ne k$, and $i, j, k = 1, ..., n$ and $n$ is the number of concepts. If the half of both this distance and the distance of weights in the opposite direction are below the design variable $\epsilon$ for all $C_k$, $C_i$ is added to cluster $K$.

\begin{figure}[hbt]
  \sidecaption
  \includegraphics[scale=1]{similarity.pdf}
  \caption{Similarity check of two concepts, $C_i$ and $C_j$ \cite{hatwagner2018two}.}
  \label{fig:similarity}
\end{figure}

\begin{algorithm}
  \caption{Function \emph{isNearB} implementing \emph{Metric ``B''}}\label{metricB}
  \begin{algorithmic}[1]
    \Function{isNearB}{$i, j, \epsilon, p$}
      \State $near \gets 0$
      \State $far \gets 0$
      \For{$k\gets0$; $k<n$; $k++$}
        \If{$k\not=i$ and $k\not=j$}
          \If{$\frac{|w_{i, k} - w_{j, k}|}{2} < \epsilon$}
            \State $near \gets near + 1$
          \Else
            \State $far \gets far + 1$
          \EndIf
          \If{$\frac{|w_{k, i} - w_{k, j}|}{2} < \epsilon$}
            \State $near \gets near + 1$
          \Else
            \State $far \gets far + 1$
          \EndIf
        \EndIf
      \EndFor
      \If{$near=0$ or $far/near \geq p$}
        \State \textbf{return} $false$
      \Else
        \State \textbf{return} $true$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Metric ``B'' (Algorithm~\ref{metricB}) is a slightly modified version of Metric ``A''. The latter works well in most cases, but sometimes a small proportion of weight differences exceed the allowed value, and prevent the merger of concepts. The second metric provides a simple solution to this problem with the usage of parameter $p$. The metric allows the merger even if the distances are greater than $\epsilon$ in a small, less than $p$ proportion of the investigated cases.

Theoretically the value of $p$ can be any in the $[0, 1]$ interval, but it should be rather low, however. High $p$ values makes possible to practically merge any concepts regardless the value of $\epsilon$, which propably leads to an unusable model.

Metric ``B'' allowed to lower the value of parameter $\epsilon$ with a simple trick, but the third approach (Metric ``C'', Algorithm~\ref{metricC}) allows the omittance of the second parameter $p$ by using the normalized, squared Euclidean distance. It is much easier to tune only one parameter instead of two in practice.

\begin{algorithm}
  \caption{Function \emph{isNearC} implementing \emph{Metric ``C''}}\label{metricC}
  \begin{algorithmic}[1]
    \Function{isNearC}{$i, j, \epsilon$}
      \State $sum \gets 0$
      \For{$k\gets0$; $k<n$; $k++$}
        \If{$k\not=i$ and $k\not=j$}
          \State $sum \gets sum + (w_{i, k} - w_{j, k})^2$
          \State $sum \gets sum + (w_{k, i} - w_{k, j})^2$
        \EndIf
      \EndFor
      \If{$\frac{sum}{(n-2)*8}<\epsilon$}
        \State \textbf{return} $true$
      \Else
        \State \textbf{return} $false$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Finally, Metric ``D'' (Algorithm~\ref{metricD}) works with the Manhattan-distance of concepts.

\begin{algorithm}
  \caption{Function \emph{isNearD} implementing \emph{Metric ``D''}}\label{metricD}
  \begin{algorithmic}[1]
    \Function{isNearD}{$i, j, \epsilon$}
      \State $sum \gets 0$
      \For{$k\gets0$; $k<n$; $k++$}
        \If{$k\not=i$ and $k\not=j$}
          \State $sum \gets sum + |w_{i, k} - w_{j, k}|$
          \State $sum \gets sum + |w_{k, i} - w_{k, j}|$
        \EndIf
      \EndFor
      \If{$\frac{sum}{(n-2)*4}<\epsilon$}
        \State \textbf{return} $true$
      \Else
        \State \textbf{return} $false$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

The applied metrics, its parameters ($\epsilon$, $p$) and also the details of implementation not specified here may affect the result of reduction. For example, if the concepts provided by $nextElement$ are in various orders, the content of clusters may be different, even if the size of the reduced model remains the same. The results should be revised by experts.

The $buildCluster$ function creates one of the clusters of the reduced model. Another function, $buildAllClusters$ (Algorithm~\ref{buildAllClusters}) calls $buildCluster$ subsequently with different $initialConcept$ values. According to the nature of the method, multiple clusters may contain the same concepts. Only one of the same clusters will be kept by the algorithm.

\begin{algorithm}
  \caption{The \emph{buildAllClusters} function}\label{buildAllClusters}
  \begin{algorithmic}[1]
    \Function{buildAllClusters}{$\epsilon$}
      \State $clusters \gets \{\}$
      \For{$i\gets0$; $i<n$; $i++$}
        \State $K \gets$ \Call{buildCluster}{$i, \epsilon$}
        \If{!\Call{isElementOf}{$K, clusters$}}
          \State $clusters \gets clusters + \{K\}$
        \EndIf
      \EndFor
      \State \textbf{return} $clusters$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Function $buildAllClusters$ returns the clusters, the concepts of the reduced model, but the weights among clusters have to be defined also. Function $getWeight$ (Algorithm~\ref{getWeight}) investigates the members of its parameter clusters, $a$ and $b$, and calculates the average (arithmetic mean) weight of relationships between concepts included in cluster $a$ to the concepts of cluster $b$. This function must be called to all possible $a$, $b$ pairs to completely define the reduced model.

\begin{algorithm}
  \caption{The \emph{getWeight} function}\label{getWeight}
  \begin{algorithmic}[1]
    \Function{getWeight}{$a, b$}
      \State $count \gets 0$
      \State $sum \gets 0$
      \While{\Call{hasNextElement}{a}}
        \State $i = \Call{nextElement}{a}$
        \While{\Call{hasNextElement}{b}}
          \State $j = \Call{nextElement}{b}$
          \If{$i\not=j$}
            \State $count \gets count + 1$
            \State $sum \gets sum + w_{i,j}$
          \EndIf
        \EndWhile
      \EndWhile
      \If{$count = 0$}
        \State \textbf{return} $0$
      \Else
        \State \textbf{return} $\frac{sum}{count}$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\cite{papageorgiou2017concept} presents step-by-step the reduction 
process of a straightforward, five-concept model in detail.

\subsection{Reduction of the motivating problem}
\label{sec:reductionIWMS}

The original model of the motivating problem (IWMS) contains 33 
concepts and 638 of the theoretically possible 1056 connections, making 
the usage of the model very cumbersome for experts. Several experiments 
were conducted with different metrics and parameter ($\epsilon, p$) 
values. This way the connection between reduction parameters and the 
evoked extent of reduction can be studied. Some interesting results are 
collected in Table~\ref{tab:reductionResults}.

\begin{table}[!t]
\caption{The number of concepts in the reduced connection matrix, using different 
metrics \cite{hatwagnerm.f.koczyl.t.2015}}
\label{tab:reductionResults}
\centering
\begin{tabular}{ccccccccc}
\hline\noalign{\smallskip}
\multicolumn{2}{c}{Metric ``A''} &
  \multicolumn{3}{c}{Metric ``B''} &
  \multicolumn{2}{c}{Metric ``C''} &
  \multicolumn{2}{c}{Metric ``D''} \\
\hline\noalign{\smallskip}
$\epsilon$ & No. of concepts &
  $\epsilon$ & p & No. of concepts &
  $\epsilon$ & No. of concepts &
  $\epsilon$ & No. of concepts \\
\noalign{\smallskip}\svhline\noalign{\smallskip}
0.3 & 28 &
  0.1 & 0.2 & 30 &
  0.01 & 30 &
  0.052 & 32 \\
0.4 & 25 &
  0.2 & 0.05 & 30 &
  0.016 & 28 &
  0.059 & 30 \\
0.5 & 18 &
  0.2 & 0.1 & 26 &
  0.022 & 24 &
  0.078 & 28 \\
0.6 & 15 &
  0.2 & 0.2 & 23 &
  0.027 & 22 &
  0.097 & 26 \\
0.7 & 12 &
  0.3 & 0.05 & 23 &
  0.04 & 20 &
  0.1 & 24 \\
0.8 & 4 &
  0.3 & 0.1 & 21 &
  0.048 & 18 &
  0.104 & 22 \\
\multicolumn{2}{c}{} & 
  0.3 & 0.2 & 15 &
  0.054 & 15 &
  0.149 & 20 \\
\multicolumn{2}{c}{} & 
  0.4 & 0.05 & 19 &
  0.06 & 12 &
  0.173 & 18 \\
\multicolumn{2}{c}{} & 
  0.4 & 0.1 & 10 &
  \multicolumn{2}{c}{} &
  0.188 & 15 \\
\multicolumn{2}{c}{} &
  \multicolumn{3}{c}{} &
  \multicolumn{2}{c}{} &
  0.2 & 13 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
\end{tabular}
\end{table}

Of course, the number of concepts after reduction does not characterize 
the usefulness of the model. 

\subsection{Theoretical background of FTR-based methods}
\label{sec:theoryFTR}

The name of the reduction methods covered in this section refer to the 
fact that they all based on Fuzzy Tolerance Relations (FTR). If a 
binary relation $R(x,x)$ is \emph{reflexive} and \emph{symmetric}, but 
\emph{not transitive} then it is called a compatibility relation in 
crisp contexts and \emph{tolerance relation} in fuzzy contexts 
\cite{klirg.j.yuanb.1995}. (All further definitions in this subsection 
are also quoted from \cite{klirg.j.yuanb.1995}.) A fuzzy binary 
relation ``$R(x,x)$ is \emph{reflexive} iff $R(x,x) = 1$ for all $x\in 
X$. A fuzzy relation is \emph{symmetric} iff $R(x,y)=R(y,x)$ for all 
$x,y\in X$''. The authors emphasize again that \emph{a FTR is never 
transitive}. (``A fuzzy relation $R(x,x)$ is transitive (\dots) if 
$R(x,z)\geq \max_{y\in Y}\min[R(x,y), R(y,z)]$ is satisfied for each 
pair $\langle x,z\rangle\in X^2$.'')

The four metrics defined by \emph{isNear} functions are all real 
distance functions. As such, the following conditions are satisfied by 
function $d$, where $d:R(X,X)\rightarrow \mathbb{R}, \forall x,y,z \in 
X$:

\begin{enumerate}
\item $d(x,x) \geq 0$
\item $d(x,y) = 0$ iff $x=y$
\item $d(x,y) = d(y,x)$ (symmetry)
\item $d(x,z) \leq d(x,y) + d(y,z)$ (triangle inequality)
\end{enumerate}

The applied metrics are symmetric functions, and \emph{isNear} 
functions always return true ($R(x,x) = 1$ or $\mu = 1$) if values of 
the parameters are the same (reflexivity). These metrics generate 
non-transitive mergers, therefore they create FTRs.

Function \emph{buildAllClusters} initiates the process of cluster 
building with every single concepts as \emph{initialConcept}. The called 
\emph{buildCluster} function may extend these initially single element 
clusters with other concepts according to the connection weights among 
concepts and the value of parameter $\epsilon$. According to this 
behavior, the following properties hold:

\begin{itemize}
  \item all concepts of the original model will be included in at least one cluster, 
  \item the same concept may be included in multiple clusters, thus 
  clusters overlap (Fig.~\ref{fig:overlapping}).
\end{itemize}

\begin{figure}[hbt]
  \sidecaption
  \includegraphics[width=7cm]{clusters.eps}
  \caption{Basic example of overlapping clusters \cite{hatwagnerm.f.koczyl.t.2015}}
  \label{fig:overlapping}
\end{figure}

\subsection{Two-Stage Learning Based Reduction}
\label{sec:2stage}

Original and reduced models obviously behave in a different way 
because the latter have less concepts, the concepts 
represent different objects of the real system, and the 
relationships among concepts must be different as well. The extent 
of differences rely primarily on the design parameter $\epsilon$ of 
model reduction. Certain differences in model behaviors do not mean 
a problem until the same decisions can be made with both models. 
Unfortunately behavioral differences are hard to measure, because 
the values of concepts representing different objects cannot be 
compared directly to each other. In order to overcome this difficulty the 
following actions were taken in \cite{hatwagner2018two}.

\subsubsection{Distinction of concept groups}
\label{sec:conceptGroups}

Three groups of concepts were distingiuished: 
\begin{enumerate}
  \item Concepts affecting other concepts but not affected by other 
  concepts are called \emph{input concepts}. They serve as the inputs 
  of the modeled system. The states of these concepts remain the same 
  during simulations, thus they neither need transformation function 
  nor its parameter $\lambda$.
  \item Some concepts are both affected by other concepts and they 
  also have an effect on more or less concepts. These are the 
  \emph{intermediate concepts}.
  \item The third group of concepts is the \emph{output 
  concepts} (or decision concepts). Their states are defined by 
  other concepts, but they do not influence the state of any other 
  concepts. 
\end{enumerate} 
In order to make the behavioral difference of original and reduced 
models (the error of simulation) measurable, a modified reduction 
method was suggested in \cite{hatwagner2018two}, that allows the merger 
of intermediate concepts only. This way simulations can be started with 
the same initial input concept states and the results in output 
concepts can be directly compared. The relation between the value of 
the design parameter $\epsilon$ and simulation error of the reduced model 
can be defined statistically using several models, many initial state 
vectors and $\epsilon$ values.

\subsubsection{Modified versions of the \emph{getWeight} function}
\label{sec:getWeightVersions}

The \emph{getWeight} function was also revised. The original version of 
it (\emph{average}, Algorithm~\ref{getWeight}) defines the weight of connection 
between two clusters ($a$ and $b$) as the arithmetic mean (average) weight of 
connections between concepts included in the corresponding clusters. 
More formally, the weight calculation can be expressed by Eq.~\ref
{eq:getWeight-average}.

\begin{equation}
  \label{eq:getWeight-average}
  w_a(a,b) = \frac{\sum_{i \in a} \sum_{j \in b} a(i,j) \times w_{ij}}{\sum_{i \in a} \sum_{j \in b} a(i,j)}
\end{equation}

\noindent where $a(i,j)$ is defined as

\begin{equation}
  \label{eq:isConnection}
  a(i,j) = \left\{ \begin{array}{ll} 
                     1 & \textrm{if } w_{ij} \neq 0 \textrm{,} \\
                     0 & \textrm{otherwise.}
                   \end{array} \right.
\end{equation}

The second approach (\emph{weighted average}) calculates the weighted 
average of connection weights among cluster members. Greater 
inter-concept weights play more important role in the definition of 
inter-cluster weights (see Eq.~\ref{eq:getWeight-weightedAverage}).

\begin{equation}
  \label{eq:getWeight-weightedAverage}
  w_w(a,b) = \frac{\sum_{i \in a} \sum_{j \in b} a(i,j) \times |w_{ij}| \times w_{ij}}{\sum_{i \in a} \sum_{j \in b} a(i,j) 
\times |w_{ij}|}
\end{equation}

The third method (\emph{extreme}, Eq.~\ref{eq:getWeight-extreme}) selects 
the connection between clusters with the maximum absolute value, and 
that will be used between the clusters. If there are conncections 
with the same absolute value but different sign, the positive one 
will be chosen.

\begin{equation}
  \label{eq:getWeight-extreme}
  w_e(a,b) = \left\{ \begin{array}{ll}
                       min(w_{ij}) & \textrm{if } \left| min(w_{ij}) \right| > \left| max(w_{ij}) \right| \\
                       & \textrm{for } \forall i \in a \wedge \forall j \in b \textrm{,} \\
                       max(w_{ij}) & \textrm{otherwise.}
                     \end{array} \right.
\end{equation}

The last method (\emph{sum}, Eq.~\ref{eq:getWeight-sum}) adds up the 
weight of connections among clusters. If that would not fit in the 
allowed $[-1,~+1]$ interval, the method rounds the weight up or 
down to the nearest allowed value.

\begin{equation}
  \label{eq:getWeight-sum}
  w_s(a,b) = \left\{ \begin{array}{ll}
                       1 & \textrm{if } s(a,b) > 1 \\
                       -1 & \textrm{if } s(a,b) < -1 \\
                       s(a,b) & \textrm{otherwise.}
                     \end{array} \right.
\end{equation}

\noindent where $s(a,b)$ is calculated as

\begin{equation}
  \label{eq:sum}
  s(a,b) = \sum_{i \in a} \sum_{j \in b} w_{ij}
\end{equation}

\subsubsection{$\lambda$ optimization}
\label{sec:lambdaOptimization}

Model reduction inevitably causes information loss and reduced 
models behave less reliable. In order to somewhat compensate this 
problem, concepts of the reduced model did not share a common 
$\lambda$ as in the original model but used different steepness 
parameters. Their values were identified by an evolutionary 
optimization method, the Big Bang -- Big Crunch (BB-BC) \cite
{yesilenginurbasleon2010} algorithm. The relatively low 
computational cost and high convergence speed made it a proper choice, 
moreover, the designer have to set only a few parameters to work 
with it. Input concepts have constant states during simulation, thus 
they do not need parameter $\lambda$ at all. Every output concept 
had its own $\lambda_{oj}$ parameter. It would have been the best if 
every intermediate concept has its own $\lambda$ value, but it 
would have increased the computational cost of the optimization 
disproportionately. As a trade-off, every intermediate concept had a 
common $\lambda_{i}$ parameter. According to these changes, the 
inference formula was modified to Eq.~\ref{eq:manyLambdaThreshold}:

\begin{equation}
  \label{eq:manyLambdaThreshold}
  A_i^{(t+1)} = f_i\left(\sum_{j=1}^{M} w_{ji}A_j^{(t)} + A_i^{(t)}\right), i \neq j
\end{equation}

\noindent where $f_i$ is a sigmoidal threshold function, and its slope is 
specified by $\lambda_i$. These parameters were identified by the 
BB-BC algorithm that minimized the objective function given by Eq.~
\ref{eq:objective}:

\begin{equation}
  \label{eq:objective}
  \sum_{i=1}^{m} a(s_i) \times \sum_{j=1}^{n} |o_{ij} - r_{ij}|
\end{equation}

Here, $m$ denotes the number of investigated \emph{scenarios} (initial 
state vectors of simulations), $s_i$ is the i\textsuperscript{th} 
scenario, and $a(s_i)$ is a function defined by Eq.~\ref{eq:isUsable}. 
It expresses the fact that we are only interested in simulations 
leading to fixed point attractors (FPs), because only they can be taken into 
account in a decision making process. $n$ is the number of output 
concepts, $o_{ij}$ and $r_{ij}$ are the values of the 
j\textsuperscript{th} output concepts at the end of the simulation of 
scenario $i$, respectively.

\begin{equation}
  \label{eq:isUsable}
  a(s_i) = \left\{ \begin{array}{ll}
                     1 & \textrm{if } s_i \textrm{ resulted in fixed point attractor in} \\
                       & \textrm{original and reduced models,} \\
                     0 & \textrm{otherwise.}
                   \end{array} \right.
\end{equation}

\subsubsection{Test environment and the method of investigation}
\label{sec:testEnvironment}

Simulation error caused by model reduction was measured by comparing 
the final values of output concepts of the original and reduced models. 
Many synthetic models were generated to describe the effect of 
reduction on a statistical basis, thereafter some models found in the 
literature were also studied.

The original synthetic models were generated randomly, taken the 
following properties in consideration: in order to meet the definitions 
in Section \ref{sec:conceptGroups}, input concepts were forced to have 
at least one outgoing connection but no input connections, and 
similarly, output concepts had to have at least one input connection 
but no outgoing connections. Intermediate concepts must had at least 
one incoming and outgoing connections, and they were not allowed to 
form islands of concepts. With other words, all intermediate concepts 
were on at least one path leading from an input to an output concept. 
This property was checked by Kruskal's well-known minimum spanning tree 
algorithm \cite{kruskal1956shortest}. Following Kosko's original idea, 
self-loops were not allowed in the generated models.

In order to imitate the diverse properties of real-life models, 
synthetic models with 20 and 30 concepts were generated. In the former 
case, there were 5 input and 3 output concepts, in the latter case 8 
input and 4 output concepts were created. The density of the connection 
matrix may have an effect on the typical model behavior, thus models 
with 10, 20, 30, \dots, 60 and 80\% densities were generated. The 
meaning of \emph{density} was defined by Eq.~\ref{eq:density}, however, 
because in our specific case some items of the connection matrix have 
to be always zero (all diagonal items, columns of input and rows of 
output concepts).

\begin{equation}
  \label{eq:density}
  d = 100 \frac{n}{\left( (c-i)(c-o) - (c-i-o) \right)}
\end{equation}

Here $n$ stands for the number of non-zero matrix elements, $c$ is the 
total number of concepts of which $i$ are input and $o$ are output concepts.

Simulations were performed with 25 model instances, thus finally 350 
original models were involved in investigations. The common $\lambda$ 
values of sigmoid threshold functions were also defined randomly for 
the test set. The simulations were started with 125 unique scenarios in 
case of all models.

Next, the reduction of models followed. Three $\epsilon$ values were 
defined for each model size--density pairs according to 
Table~\ref{tbl:epsilonDensities}. The values were referenced by their 
linguistic terms (\emph{low}, \emph{medium} and \emph{high}) instead of 
their real value, because the same parameter value causes different 
effect on models with different properties, and our goal was to cause 
comparable reduction rates.

\begin{table}[!t]
  \caption{Model densities and corresponding $\epsilon$ values with linguistic terms.}
  \label{tbl:epsilonDensities}
  \centering
  \begin{tabular}{ccccc}
    \hline\noalign{\smallskip}
    \multirow{2}{*}{No. of concepts} & \multirow{2}{*}{Density (\%)} & \multicolumn{3}{c}{$\epsilon$ values} \\
    \cline{3-5}
    & & Low & Medium & High \\
    \noalign{\smallskip}\svhline\noalign{\smallskip}
    20 & 10 & 0.06 & 0.08 & 0.10 \\
    20 & 20 & 0.10 & 0.11 & 0.13 \\
    20 & 30 & 0.13 & 0.15 & 0.16 \\
    20 & 40 & 0.16 & 0.17 & 0.18 \\
    20 & 50 & 0.18 & 0.20 & 0.21 \\
    20 & 60 & 0.21 & 0.22 & 0.24 \\
    20 & 80 & 0.24 & 0.26 & 0.28 \\
    30 & 10 & 0.06 & 0.07 & 0.08 \\
    30 & 20 & 0.11 & 0.12 & 0.14 \\
    30 & 30 & 0.14 & 0.16 & 0.18 \\
    30 & 40 & 0.18 & 0.19 & 0.20 \\
    30 & 50 & 0.20 & 0.21 & 0.22 \\
    30 & 60 & 0.22 & 0.24 & 0.26 \\
    30 & 80 & 0.26 & 0.28 & 0.30 \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
  \end{tabular}
\end{table}

The reduced models were created in four versions according to the 
various metrics applied in \emph{getWeight} versions described 
in Section \ref{sec:getWeightVersions}. 

Due to the 2 model sizes, 7 densities, 4 weight calculation methods, 3 
different reduction rates ($\epsilon$) and 25 instances with the same 
properties altogether 4200 reduced model instances were obtained.

In the first stage, reduced models inherited the $\lambda$ parameters 
of their corresponding parents. In the seconds step, however, 
$\lambda$-optimized models were created and not only the error caused 
by reduction with various $\epsilon$ parameters but the error after 
optimization were also determined.

If a simulation led to a FP at both the original and 
the corresponding reduced model, the error caused by reduction in that 
specific case was calculated by Eq.~\ref{eq:j1error}:

\begin{equation}
  \label{eq:j1error}
  J_1 = \frac{\sum_{j=1}^{n} |o_j - r_j|}{n}
\end{equation}

\noindent where $n$ is the number of output concepts, $o_j$ and $r_j$ are the 
final activation values of the j\textsuperscript{th} output concepts in 
the original and reduced models, respectively. This is a 
scenario-dependent error measure. In contrast, $J_2$ characterizes the 
model instance independently from its individual instances.

\begin{equation}
  \label{eq:j2error}
  J_2 = \frac{\sum_{i=1}^{m} J_1(i)}{m}
\end{equation}

Here $m$ denotes the number of scenarios leading to FPs. The minimum, 
lower quartile, median, upper quartile maximum, arithmetic mean and 
standard deviation of $J_2$ errors were also calculated for reduced 
models with and without $\lambda$ optimization. These error measures 
plausibly describe the information loss caused by reduction.

\subsubsection{Results of Model Reduction}
\label{sec:results}

The performed tests focused on the effect of design parameter 
$\epsilon$ on $J_2$ errors. Fig.~\ref{fig:epsilonEffect} shows the 
results of a specific case with a box-plot: the selected models have 20 
concepts, 10\% densities, and the applied values of $\epsilon$ were 
0.06, 0.08 and 0.1. $\lambda$ optimization was not carried out. Greater 
$\epsilon$ values increased the median errors and the distribution of 
errors also, but these values remained close to zero in the 
investigated cases.

\begin{figure}[hbt]
  \includegraphics[width=\textwidth]{j2_unsup_c20_d10.eps}
  \caption{The effect of $\epsilon$ on unsupervised reduction (without 
  $\lambda$ optimization): FCMs with 20 concepts, 10\% density and after 
\emph{average} weight calculation method \cite{hatwagner2018two}.}
  \label{fig:epsilonEffect}
\end{figure}

The next two figures (Fig.~\ref{fig:epsilonWeightEffect20} and 
Fig.~\ref{fig:epsilonWeightEffect30}) gives an overview of the caused 
error in case of various $\epsilon$ values, weight calculation methods 
and model densities. As it was expected, $\epsilon$ significantly 
influences the caused errors, but other interesting details can also be 
noticed: \emph{average} and \emph{sum} weight calculation methods give 
lower errors in most cases than other methods. The advantage of using 
the \emph{sum} method is especially obvious when the reduction rate is 
low. But if the model to be reduced is bigger and the intended rate of 
reduction is also greater, \emph{average} is the suggested choice. 

\begin{figure}[hbt]
  \includegraphics[width=\textwidth]{j2_unsup_c20.eps}
  \caption{The effect of weight calculation and $\epsilon$ in reduction 
  for FCMs with 20 concepts \cite{hatwagner2018two}.}
  \label{fig:epsilonWeightEffect20}
\end{figure}

\begin{figure}[hbt]
  \includegraphics[width=\textwidth]{j2_unsup_c30.eps}
  \caption{The effect of weight calculation and $\epsilon$ in reduction 
  for FCMs with 30 concepts \cite{hatwagner2018two}.}
  \label{fig:epsilonWeightEffect30}
\end{figure}

The mean and standard deviation of $J_2$ errors are easier to read in 
a tabulated form. Table~\ref{tbl:meanStddevWeightEpsilon} contains 
these data without considering the density of the model.

\begin{table}[!t]
  \caption{Mean and standard deviation obtained after the first stage of reduction for various weight calculation methods and 
$\epsilon$ values \cite{hatwagner2018two}.}
  \label{tbl:meanStddevWeightEpsilon}
  \centering
  \begin{tabular}{cccccc}
    \hline\noalign{\smallskip}
    Concepts & Epsilon ($\epsilon$) & Average 
(10\textsuperscript{-3}) & Extreme (10\textsuperscript{-3}) & Sum 
(10\textsuperscript{-3}) & Weighted average 
(10\textsuperscript{-3}) \\
    \noalign{\smallskip}\svhline\noalign{\smallskip}
    \multirow{3}{*}{20} & Low & $46 \pm 71$ & $41 \pm 62$ & $39 \pm 63$ 
    & $43 \pm 063$ \\
    & Medium & $73 \pm 97$ & $81 \pm 108$ & $73 \pm 100$ & $74 \pm 100$ 
    \\
    & High & $110 \pm 117$ & $117 \pm 129$ & $109 \pm 120$ & $114 \pm 
    129$ \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    \multirow{3}{*}{30} & Low & $63 \pm 75$ & $59 \pm 69$ & $59 \pm 72$ 
    & $59 \pm 70$ \\
    & Medium & $92 \pm 97$ & $103 \pm 106$ & $101 \pm 106$ & $97 \pm 
    104$ \\
    & High & $137 \pm 118$ & $157 \pm 138$ & $151 \pm 135$ & $144 \pm 
    128$ \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
  \end{tabular}
\end{table}

Tables~\ref{tbl:epsilonDensities}, \ref{tbl:meanStddevWeightEpsilon} and 
Figs.~\ref{fig:epsilonWeightEffect20}, \ref{fig:epsilonWeightEffect30} give 
a toolset in the hands of the decision maker. For example, if he or she 
wants to reduce a high density model with approximately 30 concepts in a high 
degree, then Table~\ref{tbl:epsilonDensities} suggests to choose the 
value of $\epsilon$ in the [0.22, 0.3] interval. Next, according to the 
chosen value the weight calculation method can be selected with the 
help of Figure~\ref{fig:epsilonWeightEffect30}: eg. \emph{average} can be a 
reasonable choice.

If the decision maker is still discontented with the results, $\lambda$ 
optimization can be a solution. This method is able to somewhat 
compensate the information loss caused by the merger of intermediate 
concepts. Figs.~\ref{fig:learning20} and \ref{fig:learning30} makes easy 
to compare the errors before and after $\lambda$ optimization of 20 and 
30 concept models, respectively. As it can be seen, this method is 
especially useful when the model is highly reduced in size. The 
differences of mean and standard deviation of $J_2$ errors are 
numerically given in Table~\ref{tbl:BBBCBeforeAfter}.

\begin{figure}[hbt]
  \includegraphics[width=\textwidth]{j2_sup_c20.eps}
  \caption{The effect of $\lambda$ optimization in reduction approach for 
  FCMs with 20 concepts \cite{hatwagner2018two}.}
  \label{fig:learning20}
\end{figure}

\begin{figure}[hbt]
  \includegraphics[width=\textwidth]{j2_sup_c30.eps}
  \caption{The effect of $\lambda$ optimization in reduction approach for 
  FCMs with 30 concepts \cite{hatwagner2018two}.}
  \label{fig:learning30}
\end{figure}

\begin{table}[!t]
  \caption{Mean and standard deviation of $J_2$ errors obtained after $\lambda$ 
  optimization for various $\epsilon$ values and \emph{sum} weight 
  calculation method \cite{hatwagner2018two}.}
  \label{tbl:BBBCBeforeAfter}
  \centering
  \begin{tabular}{cccc}
    \hline\noalign{\smallskip}
    Number of & \multirow{2}{*}{Epsilon ($\epsilon$)} & Before $\lambda$ & After $\lambda$ \\
    concepts & & optimization (10\textsuperscript{-3}) & optimization (10\textsuperscript{-3}) \\
    \noalign{\smallskip}\svhline\noalign{\smallskip}
    \multirow{3}{*}{20} & Low & $28 \pm 57$ & $25 \pm 38$ \\
    & Medium & $69 \pm 97$ & $48 \pm 62$ \\
    & High & $110 \pm 122$ & $64 \pm 76$ \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    \multirow{3}{*}{30} & Low & $55 \pm 72$ & $50 \pm 54$ \\
    & Medium & $101 \pm 105$ & $71 \pm 67$ \\
    & High & $150 \pm 134$ & $97 \pm 79$ \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
  \end{tabular}
\end{table}

\section{K-Means-based model reduction method}

The well-known K-Means clustering \cite{hartigan1979algorithm} was 
already applied to reduce FCM models in \cite{alizadeh2008using}, but 
it was used in a different way in \cite{hatwagnernovel}. The method, 
similarly to FTR-based methods, selects the concepts for merger based 
on their connection weights. K-Means needs data vectors for clustering, 
but the connection weights of an FCM are given in a matrix. Thus the 
first task is to create vectors from a matrix in the following way. The 
data vector $D_i$ of the corresponding concept $C_i$ is a row vector 
constructed from the row vector of weights representing the outgoing 
connections of $C_i$, and the transposed column vector of the weight of 
incoming connections. Formally, $\mathbf{D_i} = [w_{i,*} \;w_{*,i}]$, 
where $i \in [1, n]$ is the number of the concept, $w_{i,*}$ refers to 
the outgoing and $w_{*,i}$ to the incoming connection weights of $C_i$. 
This representation of connections makes possible for the K-Means 
algorithm to recognize the concepts with similar relationships and merge 
them in the same cluster.

In the examples of \cite{hatwagnernovel}, the models followed the 
guidelines of Kosko and self-loops were not allowed. Consequently, all 
data vectors contained at least two zero-weight items, even if the method 
itself would have been able to handle more complex models as well.

In the case of FTR-based methods, the size of the reduced model was 
controlled by $\epsilon$. In contrast, K-Means requires the number of 
clusters, but either way, the needed size of the reduced model can be 
specified. The properties of the underlying K-Means clustering causes 
concepts to appear only in exactly one of the clusters, and their 
membership value is crisp. FTR-based methods allow concepts to appear 
in multiple clusters thus it seems a drawback, but there are several 
K-Means implementations available (eg. in Matlab/Octave) making the 
KM-based method also easy to implement.

The connection weights among clusters were defined by Eq.~\ref{eq:getWeight-average}.

\section{Fuzzy C-Means-based model reduction method}

Some shortcomings of the above proposed KM-based method can be 
compensated if Fuzzy C-Means clustering \cite{yen1999fuzzy} is applied instead of K-Means.
Similarly to FTR-based methods, it allows concepts to appear in 
multiple clusters, but the sum of a concept's membership values must be 
1, and these values are not crisp (0 or 1).

Fuzzy C-Means also processes data vectors; these can be constructed 
the very same way as they were in case of the KM-based method. The size 
of the reduced model have to be defined in advance, too. The weight 
calculation method needs a slight modification, however, in order to 
take the various membership values into account (Eq.~\ref{eq:w}).

\begin{eqnarray}
  \label{eq:num} num = \sum_{i,j,i \ne j}\mu_{a,i}\mu_{b,j}w_{i,j} \\
  \label{eq:denom} denom = \sum_{i,j,i \ne j}\mu_{a,i}\mu_{b,j} \\
  \label{eq:w} w(a,b) = \left \{ 
    \begin{array}{ll}
      num/denom & if denom \ne 0 \\
      0         & otherwise \\
    \end{array}
  \right.
\end{eqnarray}

In Eqs.~\ref{eq:num}-\ref{eq:w}, $\mu_{a,i}$ denotes the membership 
value of concept $C_i$ in cluster $a$, and similarly, $\mu_{b,j}$ 
express the membership value of $C_j$ in cluster $b$. $w_{i,j}$ is the 
weight of connection between concepts $C_i$ and $C_j$, and $w(a,b)$ is 
the weight of connection between the two corresponding clusters.

\section{Comparison of the reduced models created by different methods}

First, a basic example model is reduced by KM and Fuzzy C-Means-based 
methods. This way, the operation of these methods is easy to follow, 
understand and validate the results. After that, the complex model of 
the motivating problem of waste management is reduced by all the three 
methods and some interesting properties of the reduced models are 
studied.

\subsection{Reduction of the basic model}

The model to reduce contains 4 concepts, but its reduced versions 
contain only 3 clusters. Fig.~\ref{fig:simpleExample} and 
Table~\ref{tab:connBasic} show the content of clusters, the connection 
weights among them and also the data vectors created for clustering. In 
Fig.~\ref{fig:simpleExample}.~c) the membership values less than 0.5 
are grayed out.

\begin{center}
  \begin{figure}
    \setlength\tabcolsep{.2cm}
    \begin{tabular}{p{.3\textwidth}p{.3\textwidth}p{.3\textwidth}}
      \includegraphics[width=\linewidth]{original.eps} &
      \includegraphics[width=\linewidth]{kClustered.eps} &
      \includegraphics[width=\linewidth]{fClustered.eps} \\
      \textbf{a)} The original model to be reduced. &
      \textbf{b)} Model reduced with the KM-based method. &
      \textbf{c)} Model reduced with the FCM-based method. All concepts of the original model are merged into the clusters with different membership values.\\
    \end{tabular}
    \caption{Reduction of the basic example model \cite{hatwagnernovel}.}
    \label{fig:simpleExample}
  \end{figure}
\end{center}

\begin{table}
  \caption{Connection matrix and data vectors of the basic example model \cite{hatwagnernovel}.}
  \label{tab:connBasic}
  \begin{center}
    \subfloat[Connection matrix of the basic example model.]{
      \def\arraystretch{1.1}
      \begin{tabular}{l|cccc}
               & $C_1$ & $C_2$ & $C_3$ & $C_4$ \\ \hline
         $C_1$ & 0     & 0.5   & 0     & 0 \\
         $C_2$ & -1    & 0     & -1    & 1 \\
         $C_3$ & 0     & 1     & 0     & 0 \\
         $C_4$ & 0     & -1    & 0     & 0 \\
      \end{tabular}
    }
    \hspace{1em}
    \subfloat[$D_i$ data vectors of concepts and their corresponding clusters.]{
      \def\arraystretch{1.1}
      \begin{tabular}{llcccccccc}
         \hline
         Data vector & Cluster & & & & & & \\ \hline
         $D_1$ & $K_2$ & 0     & 0.5   & 0     & 0     & 0     & -1    & 0     & 0 \\
         $D_2$ & $K_1$ & -1    & 0     & -1    & 1     & 0.5   & 0     & 1     & -1 \\
         $D_3$ & $K_2$ & 0     & 1     & 0     & 0     & 0     & -1    & 0     & 0 \\
         $D_4$ & $K_3$ & 0     & -1    & 0     & 0     & 0     & 1     & 0     & 0 \\
         \hline
      \end{tabular}
    }
  \end{center}
\end{table}

\subsection{Reduction of the motivating model}

A basic model helps understanding the operation and behavior of a 
method, but in real life, the reduction of such a simple model is not 
necessary at all. That is why, and also to compare the reduced models 
created by three different methods, the reduction of the motivating 
problem follows. The original model contains 33 concepts. It was 
reduced to 18 ($\epsilon$ = 0.048), 23 ($\epsilon$ = 0.024) and 28 
($\epsilon$ = 0.018) clusters. The details are tabulated in 
Tables~\ref{tab:clusters18}-\ref{tab:clusters28_wide}. Because of the 
limited space, exact membership values are not included in case of the 
Fuzzy C-Means-based method, concepts with greater than 0.5 
membership values are indicated as cluster members instead.

The Fuzzy C-Means-based method behaved in a very interesting way. 
First, the concepts' membership values were always very close to their 
extremes (0 or 1). For example, all ,,high'' values were at least 0.994 
in the investigated cases. Second, the default parameter value of the 
Fuzzy C-Means algorithm ($m$ = 2) resulted in models were all 
membership values were the same. In order to get useful models, the 
parameter had to be decreased to 1.1, but the algorithm not sensitive 
to slightly modified values neither. The cause of this unintended 
effect was certainly the very long, 66-element data vectors.

The clusters of the three reduced models are rather different. Experts 
should be asked about the usefulness and interpretability of these 
models, but it would be even better to carry out a detailed behavioral 
analysis and comparison in the near future. 

\begin{table}
\caption{Reduced models containing 18 clusters \cite{hatwagnernovel}.}
\label{tab:clusters18}
\resizebox{\textwidth}{!}{%
  \begin{tabular}{llll}
  \hline\noalign{\smallskip}
  Cluster & KM-based & FCM-based & FTR-based \\
  \noalign{\smallskip}\hline\noalign{\smallskip}
  K1 & C1.1+C6.5 & C1.1+C1.3+C4.7+C6.5 & C1.1+C1.2+C1.3+C2.3+C4.3+C4.4+C4.7+C5.1+C6.3 \\
  K2 & C1.2+C4.3+C6.2+C6.3+C6.4 & C1.2+C3.2 & C1.1+C1.2+C1.4+C1.5+C3.3+C4.4+C5.2+C5.3+C5.4+C6.4+C6.5 \\
  K3 & C1.3 & C1.4+C1.5 & C1.1+C1.2+C1.5+C3.3+C3.4+C3.5+C4.3+C5.4+C6.4 \\
  K4 & C1.4 & C2.1+C2.3 & C1.1+C1.2+C3.2+C3.3+C3.4+C3.5+C6.1 \\
  K5 & C1.5+C4.4+C4.5+C4.6 & C2.2 & C1.1+C1.3+C1.4+C2.5+C3.1+C3.3+C4.3+C4.4+C4.5+C4.6+C6.1 \\
  K6 & C2.1 & C2.4 & C1.1+C1.3+C1.4+C2.5+C3.1+C3.3+C4.3+C4.4+C4.6+C6.1+C6.4 \\
  K7 & C2.2 & C2.5 & C1.1+C1.3+C1.4+C3.1+C3.3+C4.3+C6.1+C6.2+C6.3+C6.4 \\
  K8 & C2.3 & C2.6 & C1.1+C1.4+C1.5+C2.5+C3.3+C3.5+C4.4+C5.2+C5.3+C6.4 \\
  K9 & C2.4 & C3.1 & C1.1+C1.4+C1.5+C4.1+C4.4+C4.5+C4.6+C5.2+C5.3 \\
  K10 & C2.5+C2.6 & C3.3+C3.5+C5.4 & C1.1+C1.4+C2.5+C3.1+C3.3+C3.5+C4.3+C4.4+C6.1+C6.4 \\
  K11 & C3.1+C3.2+C3.4 & C3.4 & C1.1+C1.4+C2.5+C4.2+C4.4+C4.5+C4.6+C5.2+C5.3 \\
  K12 & C3.3+C3.5+C3.6+C5.4 & C3.6 & C1.1+C2.3+C2.5+C3.3+C3.5+C4.3+C4.4+C5.3+C6.4 \\
  K13 & C4.1 & C4.1+C4.4 & C1.1+C3.1+C3.2+C3.3+C3.4+C3.5+C6.1 \\
  K14 & C4.2 & C4.2+C5.2+C5.3 & C1.1+C3.1+C3.3+C3.4+C3.5+C4.3+C6.1+C6.2+C6.4 \\
  K15 & C4.7 & C4.3+C6.2+C6.3+C6.4 & C1.2+C3.3+C3.4+C3.5+C3.6+C5.4 \\
  K16 & C4.7 & C4.5+C5.1 & C2.1+C2.3+C2.5+C4.4+C5.1 \\
  K17 & C5.2+C5.3 & C4.6 & C2.2+C2.3 \\
  K18 & C6.1 & C6.1 & C2.4+C2.6 \\
  \noalign{\smallskip}\hline
  \end{tabular}
}
\end{table}

\begin{table}
\caption{Reduced models containing 23 clusters \cite{hatwagnernovel}.}
\label{tab:clusters23}
\begin{center}
  \begin{tabular}{llll}
  \hline\noalign{\smallskip}
  Cluster & KM-based & FCM-based & FTR-based \\
  \noalign{\smallskip}\hline\noalign{\smallskip}
  K1 & C1.1 & C1.1 & C1.1+C1.4 \\
  K2 & C1.2 & C1.2+C6.2 & C1.1+C1.5 \\
  K3 & C1.3+C4.7 & C1.3 & C1.1+C6.5 \\
  K4 & C1.4 & C1.4+C1.5+C4.1+C4.4 & C1.2+C6.4 \\
  K5 & C1.5 & C2.1+C2.3 & C1.3+C4.7 \\
  K6 & C2.1 & C2.2 & C2.1 \\
  K7 & C2.2 & C2.4 & C2.2 \\
  K8 & C2.3 & C2.5 & C2.3 \\
  K9 & C2.4 & C2.6 & C2.4 \\
  K10 & C2.5 & C3.1 & C2.5 \\
  K11 & C2.6 & C3.2 & C2.6 \\
  K12 & C3.1 & C3.3+C3.4+C3.5 & C3.1 \\
  K13 & C3.2+C3.3+C3.5+C5.4 & C3.6 & C3.2 \\
  K14 & C3.4 & C4.2+C5.3+C5.4 & C3.3+C3.4+C3.5 \\
  K15 & C3.6 & C4.3+C6.3 & C3.6 \\
  K16 & C4.1+C4.2+C4.4+C4.5+C4.6 & C4.5 & C4.1 \\
  K17 & C4.3+C6.2+C6.3 & C4.6 & C4.2+C4.4 \\
  K18 & C5.1 & C4.7 & C4.3+C6.1+C6.2+C6.3+C6.4 \\
  K19 & C5.2 & C5.1 & C4.4+C4.5+C4.6 \\
  K20 & C5.3 & C5.2 & C5.1 \\
  K21 & C6.1 & C6.1 & C5.2 \\
  K22 & C6.4 & C6.4 & C5.3 \\
  K23 & C6.5 & C6.5 & C5.4 \\
  \noalign{\smallskip}\hline
  \end{tabular}
\end{center}
\end{table}

\begin{table}
  \caption{Reduced models containing 28 clusters \cite{hatwagnernovel}.}
  \label{tab:clusters28_wide}
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{llllp{0.3cm}llll}
      \cline{1-4} \cline{6-9} \noalign{\smallskip}
      Cluster & KM-based & FCM-based & FTR-based & &
      Cluster & KM-based & FCM-based & FTR-based \\
      \noalign{\smallskip} \cline{1-4} \cline{6-9} \noalign{\smallskip}
      K1 & C1.1+C5.1 & C1.1 & C1.1 & & K15 & C3.4 & C4.2 & C3.4 \\
      K2 & C1.2 & C1.2+C1.5 & C1.2 & & K16 & C3.5 & C4.3 & C3.6 \\
      K3 & C1.3+C4.7 & C1.3 & C1.3 & & K17 & C3.6 & C4.4 & C4.1 \\
      K4 & C1.4 & C1.4 & C1.4 & & K18 & C4.1 & C4.5 & C4.2 \\
      K5 & C1.5 & C2.1+C2.3 & C1.5 & & K19 & C4.2 & C4.6 & C4.3+C6.2+C6.3+C6.4 \\
      K6 & C2.1 & C2.2 & C2.1 & & K20 & C4.3+C6.1 & C4.7+C6.3 & C4.4+C4.5 \\
      K7 & C2.2 & C2.4 & C2.2 & & K21 & C4.4 & C5.1 & C4.5+C4.6 \\
      K8 & C2.3 & C2.5 & C2.3 & & K22 & C4.5 & C5.2 & C4.7 \\
      K9 & C2.4 & C2.6 & C2.4 & & K23 & C4.6 & C5.3 & C5.1 \\
      K10 & C2.5 & C3.1 & C2.5 & & K24 & C5.2 & C5.4 & C5.2 \\
      K11 & C2.6 & C3.2 & C2.6 & & K25 & C5.3 & C6.1 & C5.3 \\
      K12 & C3.1 & C3.3+C3.4 & C3.1 & & K26 & C6.2 & C6.2 & C5.4 \\
      K13 & C3.2 & C3.5+C3.6 & C3.2 & & K27 & C6.3+C6.4 & C6.4 & C6.1 \\
      K14 & C3.3+C5.4  & C4.1 & C3.3+C3.5 & & K28 & C6.5 & C6.5 & C6.5 \\
      \noalign{\smallskip} \cline{1-4} \cline{6-9}
    \end{tabular}
  }
\end{table}

\begin{acknowledgement}
If you want to include acknowledgments of assistance and the like at 
the end of an individual chapter please use the \verb|acknowledgement| 
environment -- it will automatically be rendered in line with the preferred layout.
\end{acknowledgement}

\input{references}
\end{document}
