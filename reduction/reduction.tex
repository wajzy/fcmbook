%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a contributed volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[graybox]{svmult}

\usepackage[utf8]{inputenc}

% choose options for [] as required from the list
% in the Reference Guide

\usepackage{type1cm}        % activate if the above 3 fonts are
                            % not available on your system
%
\usepackage{makeidx}         % allows index generation
\usepackage{graphicx}        % standard LaTeX graphics tool
                             % when including figure files
\usepackage{multicol}        % used for the two-column index
\usepackage[bottom]{footmisc}% places footnotes at page bottom


\usepackage{newtxtext}       % 
\usepackage{newtxmath}       % selects Times Roman as basic font

\usepackage{algorithm}
\usepackage{algpseudocode}

% see the list of further useful packages
% in the Reference Guide

\makeindex             % used for the subject index
                       % please use the style svind.ist with
                       % your makeindex program

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title*{Concept Reduction Methods}
% Use \titlerunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\author{Miklós F. Hatwagner}
% Use \authorrunning{Short Title} for an abbreviated version of
% your contribution title if the original one is too long
\institute{Miklós F. Hatwagner \at Széchenyi István University, Győr, Hungary \email{miklos.hatwagner@sze.hu}}
%
% Use the package "url.sty" to avoid
% problems with special characters
% used in your e-mail or web address
%
\maketitle

\abstract*{<To be prepared>}

\abstract{<To be prepared>}

\section{The Motivating Problem}
\label{sec:1}

The title of Adrienn Buruzs's PhD thesis \cite{buruzsphd2015} is ``Evaluation of Sustainable Regional Waste Management Systems with Fuzzy Cognitive Map''. As the title suggests, she analyzed the internal driving forces, dynamic behavior and sustainability of Integrated Waste Management Systems (IWMSs), which are very complex systems including many aspects (environmental, economic, social, institutional, legal and technical) and stakeholders. Even at an early stage of her investigations became apparent that Fuzzy Cognitive Map (FCM) is an appropriate tool to describe the large number of interacting and coupled entities and it copes with the inherent uncertainties of the system.

At first, a new FCM model \cite{buruzs2013developing} was created, which contained six concepts. These concepts were identified on the basis of the literature. The strength of relationships among concepts were defined by the results of a survey filled out by 75 stakeholders. The simulation results provided by FCM were validated later in \cite{buruzs2013advanced}. Time series data were collected based on the relevant literature and it served as the input of a Bacterial Evolutionary Algorithm to learn the connection weights among the already specified concepts and parameter $\lambda$ of the threshold function. The goal of optimization was to find an FCM that generates as similar time series as possible. Unfortunately, a strong contradiction was explored between the models created by experts and machine learning.

In order to resolve the experienced problem the concepts of the original model were decomposed to further 4-7 sub-concepts according to the System-of-Systems approach, which led to a very detailed, completely new model of IWMS \cite{buruzs2013modeling}. A workshop was organized with the help of 12 stakeholders who decided the sub-concepts and their interconnections. The result of their work is a FCM containing 33 concepts in total (Fig.~\ref{fig:flower}). Unfortunately such an extremely complex model is often confusing for the experts (Fig.~\ref{fig:fcmbig}), and to work with them  may be very laborious. Note that the number of connections is a quadratic function of the number of concepts.

That is why, in general, the following approach is suggested to follow in practice: start with an obviously oversized, fine-grained model. Experts are often uncertain about the importance of system components thus it worth include most or all of them in the preliminary model. Then start reducing the model automatically, in an algorithmic way until the balance of model size and required accuracy is found. The numerical accuracy of reduced models are always lower by their nature, but it does not cause a problem in practice until the decisions suggested by them are the same. On the other hand, simpler models are easier to understand, their visual representation is clearer, and it is often more important for experts and managers. In the following sections several possible ways of model reduction is presented.

\begin{figure}[hbt]
  \begin{center}
    \includegraphics[width=\textwidth]{szines_virag.pdf}
  \end{center}
  \caption{The main concepts and their sub-concepts of regional IWMS.}
  \label{fig:flower}
\end{figure}

\begin{figure}[hbt]
  \begin{center}
    \includegraphics[width=\textwidth]{fcm_big.pdf}
  \end{center}
  \caption{The 33 concept model of regional IWMS and the relationships among its concepts. It is hard to illustrate complex FCM models appropriately and graphical model visualizations often confuse experts.}
  \label{fig:fcmbig}
\end{figure}

\section{Early model reduction methods}
\label{sec:2}

Several methods had been suggested to solve the problem of oversized models before the complex model of IWMS saw the light of the day. These approaches are based on different perspectives.

In \cite{alizadeh2008using} an FCM is learned using historical data and its concepts are grouped into clusters in a unique way. The clustering is based on the DEMATEL \cite{dematel} method. The concepts are arranged on a two dimensional plot. The vertical axis classifies concepts to cause and effect groups, the position of concepts along the horizontal axis expresses the importance of them. Based on this rearrangement of concepts two clustering methods are suggested. The first one uses K-Means clustering to create clusters according to the cause-effect behavior of concepts. The cluster centers replace the original concepts in the reduced model. The second method contains two consecutive steps, and takes also the importance of concepts into consideration. Regardless of the methods applied, experts have to define the number of clusters and they must be disjoint.

FCM was used to predict and discover knowledge about the HIV-1 drug resistance in \cite{NapolesHIV}. The protease protein was modeled by FCM and causalities among sequence positions were estimated by Particle Swarm Optimization. Furthermore, Ant Colony Optimization was also applied in order to find the strongest sequence positions related to the resistance target. After that, some concepts and their connections were removed to decrease the complexity of the model until the quality of inference remained acceptable.

Another reduction method was introduced in \cite{Homenda2014}. Weak concepts and their connections were removed, then the inference capabilities of the simplified model were tested with time series data collected from real-world applications. The reduced model replaced the original if its estimation errors were acceptable.

\section{Fuzzy Tolerance Relations-based reduction methods}
\label{sec:2}

The first results on a novel state reduction method family were presented in \cite{hatwagner2014strategic,hatwagner2015new}. The members of this family differ only in the applied metric, which is used to measure the ``distance'' of concepts from each other. The methods may be considered as generalizations of the state reduction of finite state machines and sequential systems with partially defined states. They are widely applied in digital design where the complexity of the problem makes it possible \cite{kohaviz.jhan.k.2009}. The common idea is to create clusters of identical or similar concepts, and use these clusters instead of their members in the reduced model. The methods are based on Fuzzy Tolerance Relations (FTR), which is an extension of compatibility relations among crisp concepts. The methods examine the connection weights among concepts, because they define the effect on other concepts.

At the very beginning the new model contains exactly as many clusters as the number of concepts in the original model. These clusters are all disjoint single element sets containing one of the original concepts. In the following, the $i^{th}$ concept is denoted by $C_i$, and similarly, the $i^{th}$ cluster is denoted by $K_i$. The number of concepts is $n$. In the next steps further concepts are added to the clusters if they are ``close enough'' to each member of the cluster. The ``distance'' of concepts can be measured by various metrics. These metrics can be selected according to the specialities of the problem, and they differentiate the members of the algorithm family. Finally, some of these expanded clusters may be identical, containing exactly the same concepts. In order to keep clusters unique, only one of these clusters is retained.

One of the main functions is called $buildCluster$ (Algorithm~\ref{buildCluster}). Its goal is to create a cluster that initially contains only its $initialConcept$. Later this cluster will be expanded by merger of other concepts. The second parameter, $\epsilon$ specifies the maximum allowed distance between current cluster members and the concept under examination. The value of $\epsilon$ must be in the [0,~1] interval. This parameter plays an important role in model reduction, and have to be chosen properly by experts. Low values hardly reduce the size of the model, but high values may cause oversimplification. Its appropriate value is completely problem dependent.

\begin{algorithm}
  \caption{The \emph{buildCluster} function}\label{buildCluster}
  \begin{algorithmic}[1]
    \Function{buildCluster}{$initialConcept,\epsilon$}
    \State $K\gets \{initialConcept\}$
    \For{$i\gets0$; $i<n$; $i++$}
      \If{$i \not= initialConcept$}
        \State $member \gets true$
        \While{$member$ and \Call{hasNextElement}{$K$}}
          \State $j \gets$ \Call{nextElement}{$K$}
          \State $member \gets$ \Call{isNearA}{$j, i, \epsilon$}
        \EndWhile
        \If{$member$}
          \State $K \gets K + \{i\}$
        \EndIf
      \EndIf
    \EndFor
    \State \textbf{return} $c$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Function $isNearA$ is called several times in $buildCluster$ to decide whether the current concept $C_i$ can become a member of cluster $K$ or not. The number of function calls depends on how many member concepts do the cluster already have. This function (Algorithm~\ref{metricA}) implements one of the possible metrics, but can be repleced by any of the metrics presented here. The last one was suggested in \cite{hatwagnerm.f.koczyl.t.2015}.

\begin{algorithm}
  \caption{Function \emph{isNearA} implementing \emph{Metric ``A''}}\label{metricA}
  \begin{algorithmic}[1]
    \Function{isNearA}{$i, j, \epsilon$}
      \State $near \gets true$
      \For{$k\gets0$; $k<n$ and $near = true$; $k++$}
        \If{$k\not=i$ and $k\not=j$}
          \If{$\frac{|w_{i, k} - w_{j, k}|}{2} \geq \epsilon$ or 
              $\frac{|w_{k, i} - w_{k, j}|}{2} \geq \epsilon$} 
            \State $near \gets false$
          \EndIf
        \EndIf
      \EndFor
      \State \textbf{return} $near$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Metric ``A'' calculates the absolute difference of two connection weights, $w_{i,k}$ and $w_{j,k}$ from concept $C_i$, a cluster member candidate and concept $C_j$, a concept of cluster $K$ to a third concept $C_k$, where $i \ne j \ne k$, and $i, j, k = 1, ..., n$ and $n$ is the number of concepts. If the half of both this distance and the distance of weights in the opposite direction are below the design variable $\epsilon$ for all $C_k$, $C_i$ is added to cluster $K$.

\begin{algorithm}
  \caption{Function \emph{isNearB} implementing \emph{Metric ``B''}}\label{metricB}
  \begin{algorithmic}[1]
    \Function{isNearB}{$i, j, \epsilon, p$}
      \State $near \gets 0$
      \State $far \gets 0$
      \For{$k\gets0$; $k<n$; $k++$}
        \If{$k\not=i$ and $k\not=j$}
          \If{$\frac{|w_{i, k} - w_{j, k}|}{2} < \epsilon$}
            \State $near \gets near + 1$
          \Else
            \State $far \gets far + 1$
          \EndIf
          \If{$\frac{|w_{k, i} - w_{k, j}|}{2} < \epsilon$}
            \State $near \gets near + 1$
          \Else
            \State $far \gets far + 1$
          \EndIf
        \EndIf
      \EndFor
      \If{$near=0$ or $far/near \geq p$}
        \State \textbf{return} $false$
      \Else
        \State \textbf{return} $true$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Metric ``B'' (Algorithm~\ref{metricB}) is a slightly modified version of Metric ``A''. The latter works well in most cases, but sometimes a small proportion of weight differences exceed the allowed value, and prevent the merger of concepts. The second metric provides a simple solution to this problem with the usage of parameter $p$. The metric allows the merger even if the distances are greater than $\epsilon$ in a small, less than $p$ proportion of the investigated cases.

Theoretically the value of $p$ can be any in the $[0, 1]$ interval, but it should be rather low, however. High $p$ values makes possible to practically merge any concepts regardless the value of $\epsilon$, which propably leads to an unusable model.

Metric ``B'' allowed to lower the value of parameter $\epsilon$ with a simple trick, but the third approach (Metric ``C'', Algorithm\ref{metricC}) allows the omittance of the second parameter $p$ by using the normalized, squared Euclidean distance. It is much easier to tune only one parameter instead of two in practice.

\begin{algorithm}
  \caption{Function \emph{isNearC} implementing \emph{Metric ``C''}}\label{metricC}
  \begin{algorithmic}[1]
    \Function{isNearC}{$i, j, \epsilon$}
      \State $sum \gets 0$
      \For{$k\gets0$; $k<n$; $k++$}
        \If{$k\not=i$ and $k\not=j$}
          \State $sum \gets sum + (w_{i, k} - w_{j, k})^2$
          \State $sum \gets sum + (w_{k, i} - w_{k, j})^2$
        \EndIf
      \EndFor
      \If{$\frac{sum}{(n-2)*8}<\epsilon$}
        \State \textbf{return} $true$
      \Else
        \State \textbf{return} $false$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Finally, Metric ``D'' (Algorithm~\ref{metricD}) works with the Manhattan-distance of concepts.

\begin{algorithm}
  \caption{Function \emph{isNearD} implementing \emph{Metric ``D''}}\label{metricD}
  \begin{algorithmic}[1]
    \Function{isNearD}{$i, j, \epsilon$}
      \State $sum \gets 0$
      \For{$k\gets0$; $k<n$; $k++$}
        \If{$k\not=i$ and $k\not=j$}
          \State $sum \gets sum + |w_{i, k} - w_{j, k}|$
          \State $sum \gets sum + |w_{k, i} - w_{k, j}|$
        \EndIf
      \EndFor
      \If{$\frac{sum}{(n-2)*4}<\epsilon$}
        \State \textbf{return} $true$
      \Else
        \State \textbf{return} $false$
      \EndIf
    \EndFunction
  \end{algorithmic}
\end{algorithm}

\begin{acknowledgement}
If you want to include acknowledgments of assistance and the like at the end of an individual chapter please use the \verb|acknowledgement| environment -- it will automatically be rendered in line with the preferred layout.
\end{acknowledgement}

\input{references}
\end{document}
